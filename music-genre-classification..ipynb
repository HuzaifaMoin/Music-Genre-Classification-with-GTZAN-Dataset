{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Music Genre Classification with GTZAN Dataset\n",
    "\n",
    "## Overview\n",
    "This notebook implements a music genre classification system using the GTZAN dataset. The goal is to classify audio clips into 10 genres (e.g., Blues, Classical, Rock) using two approaches: a **tabular approach** with pre-extracted audio features and an **image-based approach** with spectrograms processed by a convolutional neural network (CNN). The task compares these approaches to evaluate their performance and explores transfer learning as a bonus.\n",
    "\n",
    "## Dataset\n",
    "The GTZAN dataset contains:\n",
    "- **Audio Files**: 1000 WAV files (30 seconds each) across 10 genres in the `genres_original` directory.\n",
    "- **Features**: Pre-extracted features (e.g., MFCCs, chroma, tempo) in `features_30_sec.csv` for tabular classification.\n",
    "\n",
    "## Approaches\n",
    "1. **Tabular Approach**: Uses pre-extracted features with a dense neural network to classify genres.\n",
    "2. **Image-Based Approach**: Generates Mel-spectrograms from audio files and trains a CNN for classification.\n",
    "3. **Comparison**: Evaluates both approaches using accuracy, classification reports, and interactive confusion matrices.\n",
    "\n",
    "## Tools\n",
    "- **Python**: Core programming language.\n",
    "- **Librosa**: For audio processing (MFCCs, spectrograms).\n",
    "- **Scikit-learn**: For preprocessing and metrics.\n",
    "- **Keras/TensorFlow**: For neural network models.\n",
    "- **Plotly**: For interactive visualizations.\n",
    "\n",
    "## Goals\n",
    "- Achieve high classification accuracy for both approaches.\n",
    "- Compare tabular and image-based models to understand their strengths and weaknesses.\n",
    "- Optionally explore transfer learning (e.g., VGG16) for the image-based approach.\n",
    "\n",
    "---\n",
    "\n",
    "## Imports\n",
    "Import all necessary Python libraries here.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "%matplotlib inline\n",
    "import sklearn.metrics as skm\n",
    "import sklearn.model_selection as skms\n",
    "import sklearn.preprocessing as skp\n",
    "import random\n",
    "import librosa, IPython\n",
    "import librosa.display as lplt\n",
    "seed = 12\n",
    "np.random.seed(seed)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Load and Inspect Data\n",
    "Load your dataset and take a first look at its structure.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "df = pd.read_csv('archive (3)/Data/features_3_sec.csv')\n",
    "df.head()\n",
    "\n",
    "print(\"Dataset has\",df.shape)\n",
    "\n",
    "df.columns\n",
    "\n",
    "audio_path = 'archive (3)/Data/genres_original/blues/blues.00000.wav'\n",
    "audio_data, sr = librosa.load(audio_path,sr=None)\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "lplt.waveshow(audio_data, sr=sr)  # Updated function\n",
    "plt.title(\"Waveform\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.show()\n",
    "\n",
    "# plot zoomed audio wave \n",
    "start = 1000\n",
    "end = 1200\n",
    "plt.figure(figsize=(16,4))\n",
    "plt.plot(audio_data[start:end])\n",
    "plt.show()\n",
    "\n",
    "---\n",
    "\n",
    "## Exploratory Data Analysis (EDA)\n",
    "Visualize and summarize the data to uncover patterns and insights.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "# Computing the Correlation Matrix\n",
    "spike_cols = [col for col in df.columns if 'mean' in col]\n",
    "corr = df[spike_cols].corr()\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))  # Changed from np.bool\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(25, 15))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(0, 25, as_cmap=True, s=90, l=45, n=5)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr,mask=mask,cmap=cmap, vmax=.3, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True)\n",
    "\n",
    "plt.title('Correlation Heatmap (for the MEAN variables)', fontsize=20)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.savefig(\"Corr_Heatmap.png\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "The heatmap above shows the pairwise correlations between the mean values of various audio features. Darker shades of red indicate stronger negative correlations, while lighter shades represent weaker or positive correlations. This helps identify relationships between features, such as highly correlated MFCC coefficients.\n",
    "\n",
    "x = df[[\"label\", \"tempo\"]]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 8));\n",
    "sns.boxplot(x = \"label\", y = \"tempo\", data = x, palette = 'husl');\n",
    "\n",
    "plt.title('BPM Boxplot for Genres', fontsize = 20)\n",
    "plt.xticks(fontsize = 14)\n",
    "plt.yticks(fontsize = 10);\n",
    "plt.xlabel(\"Genre\", fontsize = 15)\n",
    "plt.ylabel(\"BPM\", fontsize = 15)\n",
    "plt.savefig(\"BPM_Boxplot.png\")\n",
    "plt.show()\n",
    "\n",
    "The boxplot above compares BPM distributions across music genres. Each box shows the median, quartiles, and spread of BPM values, while dots represent outliers. This visualization highlights differences in tempo characteristics among genres.\n",
    "\n",
    "# map labels to index\n",
    "label_index = dict()\n",
    "index_label = dict()\n",
    "for i, x in enumerate(df.label.unique()):\n",
    "    label_index[x] = i\n",
    "    index_label[i] = x\n",
    "print(label_index)\n",
    "print(index_label)\n",
    "\n",
    "---\n",
    "\n",
    "# Data Preparation and Model Evaluation\n",
    "\n",
    "Preparing the audio dataset and evaluating model performance for music genre classification. Data preparation involves cleaning, transforming, and engineering audio features to ensure compatibility with classification models. Model evaluation uses appropriate metrics (e.g., accuracy, precision, recall, F1-score) to assess the modelâ€™s ability to correctly identify the genre of a given sound sample.\n",
    "\n",
    "---\n",
    "\n",
    "df.label = [label_index[l] for l in df.label]\n",
    "df.label\n",
    "\n",
    "# shuffle samples\n",
    "df_shuffle = df.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "\n",
    "# remove irrelevant columns\n",
    "df_shuffle.drop(['filename', 'length'], axis=1, inplace=True)\n",
    "df_y = df_shuffle.pop('label')\n",
    "df_X = df_shuffle\n",
    "\n",
    "# split into train dev and test\n",
    "X_train, df_test_valid_X, y_train, df_test_valid_y = skms.train_test_split(df_X, df_y, train_size=0.7, random_state=seed, stratify=df_y)\n",
    "X_dev, X_test, y_dev, y_test = skms.train_test_split(df_test_valid_X, df_test_valid_y, train_size=0.66, random_state=seed, stratify=df_test_valid_y)\n",
    "\n",
    "print(f\"Train set has {X_train.shape[0]} records out of {len(df_shuffle)} which is {round(X_train.shape[0]/len(df_shuffle)*100)}%\")\n",
    "print(f\"Dev set has {X_dev.shape[0]} records out of {len(df_shuffle)} which is {round(X_dev.shape[0]/len(df_shuffle)*100)}%\")\n",
    "print(f\"Test set has {X_test.shape[0]} records out of {len(df_shuffle)} which is {round(X_test.shape[0]/len(df_shuffle)*100)}%\")\n",
    "\n",
    "scaler = skp.StandardScaler()\n",
    "X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n",
    "X_dev = pd.DataFrame(scaler.transform(X_dev), columns=X_train.columns)\n",
    "X_test = pd.DataFrame(scaler.transform(X_test), columns=X_train.columns)\n",
    "\n",
    "---\n",
    "\n",
    "### Model Training with TensorFlow and Keras\n",
    "\n",
    "This section defines and trains a neural network for music genre classification using TensorFlow and Keras. The model includes an early stopping callback that halts training once validation accuracy exceeds a predefined threshold, preventing overfitting and saving computation time. Training history is plotted to visualize performance over epochs.\n",
    "\n",
    "---\n",
    "\n",
    "import tensorflow as tf\n",
    "print(\"TF version:-\", tf.__version__)\n",
    "import keras as k\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "ACCURACY_THRESHOLD = 0.94\n",
    "\n",
    "class myCallback(k.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if(logs.get('val_accuracy') > ACCURACY_THRESHOLD):\n",
    "            print(\"\\n\\nStopping training as we have reached %2.2f%% accuracy!\" %(ACCURACY_THRESHOLD*100))   \n",
    "            self.model.stop_training = True\n",
    "\n",
    "def trainModel(model, epochs, optimizer):\n",
    "    batch_size = 128\n",
    "    callback = myCallback()\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']  # fixed here\n",
    "    )\n",
    "    return model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_dev, y_dev),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        callbacks=[callback]\n",
    "    )\n",
    "\n",
    "def plotHistory(history):\n",
    "    print(\"Max. Validation Accuracy\",max(history.history[\"val_accuracy\"]))\n",
    "    pd.DataFrame(history.history).plot(figsize=(12,6))\n",
    "    plt.show()\n",
    "\n",
    "model_1 = k.models.Sequential([\n",
    "    k.layers.Dense(256, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    k.layers.Dense(128, activation='relu'),\n",
    "    k.layers.Dense(64, activation='relu'),\n",
    "    k.layers.Dense(10, activation='softmax'),\n",
    "])\n",
    "print(model_1.summary())\n",
    "model_1_history = trainModel(model=model_1, epochs=11, optimizer='adam')\n",
    "\n",
    "plotHistory(model_1_history)\n",
    "\n",
    "model_2 = k.models.Sequential([\n",
    "    k.layers.Dense(512, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    k.layers.Dropout(0.2),\n",
    "    \n",
    "    k.layers.Dense(256, activation='relu'),\n",
    "    k.layers.Dropout(0.2),\n",
    "\n",
    "    k.layers.Dense(128, activation='relu'),\n",
    "    k.layers.Dropout(0.2),\n",
    "\n",
    "    k.layers.Dense(64, activation='relu'),\n",
    "    k.layers.Dropout(0.2),\n",
    "\n",
    "    k.layers.Dense(10, activation='softmax'),\n",
    "])\n",
    "print(model_2.summary())\n",
    "model_2_history = trainModel(model=model_2, epochs=20, optimizer='adam')\n",
    "\n",
    "plotHistory(model_2_history)\n",
    "\n",
    "\n",
    "model_3 = k.models.Sequential([\n",
    "    k.layers.Dense(512, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    k.layers.Dropout(0.2),\n",
    "    \n",
    "    k.layers.Dense(256, activation='relu'),\n",
    "    k.layers.Dropout(0.2),\n",
    "\n",
    "    k.layers.Dense(128, activation='relu'),\n",
    "    k.layers.Dropout(0.2),\n",
    "\n",
    "    k.layers.Dense(64, activation='relu'),\n",
    "    k.layers.Dropout(0.2),\n",
    "\n",
    "    k.layers.Dense(10, activation='softmax'),\n",
    "])\n",
    "print(model_3.summary())\n",
    "model_3_history = trainModel(model=model_3, epochs=290, optimizer='sgd')\n",
    "\n",
    "plotHistory(model_3_history)\n",
    "\n",
    "\n",
    "model_4 = k.models.Sequential([\n",
    "    k.layers.Dense(1024, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    k.layers.Dropout(0.3),\n",
    "    \n",
    "    k.layers.Dense(512, activation='relu'),\n",
    "    k.layers.Dropout(0.3),\n",
    "\n",
    "    k.layers.Dense(256, activation='relu'),\n",
    "    k.layers.Dropout(0.3),\n",
    "\n",
    "    k.layers.Dense(128, activation='relu'),\n",
    "    k.layers.Dropout(0.3),\n",
    "\n",
    "    k.layers.Dense(64, activation='relu'),\n",
    "    k.layers.Dropout(0.3),\n",
    "\n",
    "    k.layers.Dense(10, activation='softmax'),\n",
    "])\n",
    "print(model_4.summary())\n",
    "model_4_history = trainModel(model=model_4, epochs=30, optimizer='rmsprop')\n",
    "\n",
    "plotHistory(model_4_history)\n",
    "\n",
    "\n",
    "test_loss, test_acc  = model_4.evaluate(X_test, y_test, batch_size=128)\n",
    "print(\"The test Loss is :\",test_loss)\n",
    "print(\"\\nThe Best test Accuracy is :\",test_acc*100)\n",
    "\n",
    "---\n",
    "\n",
    "### Spectrogram Generation and CNN Training\n",
    "\n",
    "This section converts audio files into mel-spectrograms, optionally applying data augmentation through time and frequency masking to improve model generalization. The processed spectrograms are used to train a Convolutional Neural Network (CNN) for music genre classification. The model architecture includes multiple convolutional and pooling layers, dropout for regularization, and early stopping to prevent overfitting. Performance is evaluated on a held-out test set.\n",
    "\n",
    "---\n",
    "\n",
    "def create_spectrogram(file_path, img_size=(128, 128), augment=False):\n",
    "    try:\n",
    "        y, sr = librosa.load(file_path, duration=30)  # Load first 30 seconds\n",
    "        mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128)\n",
    "        mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)  # Convert to dB\n",
    "        # Normalize to [0, 1]\n",
    "        mel_spec_db = (mel_spec_db - mel_spec_db.min()) / (mel_spec_db.max() - mel_spec_db.min())\n",
    "        mel_spec_db = tf.image.resize(mel_spec_db[..., np.newaxis], img_size).numpy().squeeze()\n",
    "        \n",
    "        if augment:\n",
    "            # Milder time and frequency masking (5% instead of 10%)\n",
    "            if np.random.rand() > 0.5:\n",
    "                max_mask = int(mel_spec_db.shape[1] * 0.05)\n",
    "                t_start = np.random.randint(0, mel_spec_db.shape[1] - max_mask)\n",
    "                mel_spec_db[:, t_start:t_start + max_mask] = 0\n",
    "            if np.random.rand() > 0.5:\n",
    "                max_mask = int(mel_spec_db.shape[0] * 0.05)\n",
    "                f_start = np.random.randint(0, mel_spec_db.shape[0] - max_mask)\n",
    "                mel_spec_db[f_start:f_start + max_mask, :] = 0\n",
    "        \n",
    "        return mel_spec_db\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Load audio files from genres_original\n",
    "data_path = 'archive (3)/Data'\n",
    "genres_path = os.path.join(data_path, 'genres_original')\n",
    "data = []\n",
    "labels = []\n",
    "img_size = (128, 128)\n",
    "genres = os.listdir(genres_path)\n",
    "\n",
    "for genre in genres:\n",
    "    genre_path = os.path.join(genres_path, genre)\n",
    "    for file in os.listdir(genre_path):\n",
    "        if file.endswith('.wav'):\n",
    "            file_path = os.path.join(genre_path, file)\n",
    "            spec = create_spectrogram(file_path, img_size, augment=True)\n",
    "            if spec is not None:\n",
    "                data.append(spec)\n",
    "                labels.append(genre)\n",
    "\n",
    "# Convert to arrays\n",
    "X_cnn = np.array(data)[..., np.newaxis]  # Add channel dimension\n",
    "y_cnn = np.array([label_index[l] for l in labels])  # Reuse label_index\n",
    "\n",
    "# Split data (70% train, ~20% dev, ~10% test)\n",
    "X_train_cnn, X_test_valid_cnn, y_train_cnn, y_test_valid_cnn = train_test_split(\n",
    "    X_cnn, y_cnn, train_size=0.7, random_state=42, stratify=y_cnn\n",
    ")\n",
    "X_dev_cnn, X_test_cnn, y_dev_cnn, y_test_cnn = train_test_split(\n",
    "    X_test_valid_cnn, y_test_valid_cnn, train_size=0.66, random_state=42, stratify=y_test_valid_cnn\n",
    ")\n",
    "\n",
    "print(f\"CNN Train set: {X_train_cnn.shape[0]} samples\")\n",
    "print(f\"CNN Dev set: {X_dev_cnn.shape[0]} samples\")\n",
    "print(f\"CNN Test set: {X_test_cnn.shape[0]} samples\")\n",
    "\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Define improved CNN\n",
    "cnn_model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 1)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Dropout(0.4),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.4),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile with lower learning rate\n",
    "cnn_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "cnn_model.summary()\n",
    "\n",
    "# Train with early stopping\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "cnn_history = cnn_model.fit(\n",
    "    X_train_cnn, y_train_cnn,\n",
    "    validation_data=(X_dev_cnn, y_dev_cnn),\n",
    "    epochs=50, batch_size=32,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate on test set\n",
    "test_loss_cnn, test_acc_cnn = cnn_model.evaluate(X_test_cnn, y_test_cnn)\n",
    "print(f\"Improved CNN Test Accuracy: {test_acc_cnn*100:.2f}%\")\n",
    "\n",
    "# Generate predictions\n",
    "y_pred_cnn = cnn_model.predict(X_test_cnn).argmax(axis=1)\n",
    "\n",
    "# Classification report\n",
    "print(\"CNN Classification Report:\")\n",
    "print(classification_report(y_test_cnn, y_pred_cnn, target_names=label_index.keys()))\n",
    "\n",
    "plt.style.use('seaborn-v0_8')  # or just 'seaborn' in older versions\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Predictions for Model_4 (tabular)\n",
    "y_pred_model_4 = model_4.predict(X_test, batch_size=128).argmax(axis=1)\n",
    "\n",
    "# Ensure consistent label names (reuse label_index from tabular code)\n",
    "genres = list(label_index.keys())\n",
    "\n",
    "# Create subplots for confusion matrices\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "models_data = [\n",
    "    (\"Dense Neural Network (Model_4)\", y_test, y_pred_model_4),\n",
    "    (\"Simple CNN\", y_test_cnn, y_pred_cnn)\n",
    "]\n",
    "\n",
    "# Plot confusion matrices\n",
    "for idx, (name, y_true, y_pred) in enumerate(models_data):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # Create heatmap\n",
    "    sns.heatmap(cm, \n",
    "                annot=True,           # Show numbers in cells\n",
    "                fmt='d',              # Integer format\n",
    "                cmap='Blues',         # Color scheme\n",
    "                xticklabels=genres,   # X-axis labels\n",
    "                yticklabels=genres,   # Y-axis labels\n",
    "                ax=axes[idx],         # Specify subplot\n",
    "                cbar_kws={'shrink': 0.8})  # Colorbar size\n",
    "    \n",
    "    axes[idx].set_title(f'Confusion Matrix: {name}', fontsize=14, fontweight='bold')\n",
    "    axes[idx].set_xlabel('Predicted Genre', fontsize=12)\n",
    "    axes[idx].set_ylabel('True Genre', fontsize=12)\n",
    "    axes[idx].tick_params(axis='x', rotation=45)\n",
    "    axes[idx].tick_params(axis='y', rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Training history comparison\n",
    "history_df_model_4 = pd.DataFrame(model_4_history.history)\n",
    "history_df_cnn = pd.DataFrame(cnn_history.history)\n",
    "\n",
    "# Create training history plot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot accuracy\n",
    "ax1.plot(history_df_model_4.index, history_df_model_4['accuracy'], \n",
    "         label='Model_4 Train', linewidth=2, marker='o', markersize=4)\n",
    "ax1.plot(history_df_model_4.index, history_df_model_4['val_accuracy'], \n",
    "         label='Model_4 Validation', linewidth=2, marker='s', markersize=4)\n",
    "ax1.plot(history_df_cnn.index, history_df_cnn['accuracy'], \n",
    "         label='CNN Train', linewidth=2, marker='^', markersize=4)\n",
    "ax1.plot(history_df_cnn.index, history_df_cnn['val_accuracy'], \n",
    "         label='CNN Validation', linewidth=2, marker='D', markersize=4)\n",
    "\n",
    "ax1.set_title('Model Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Epoch', fontsize=12)\n",
    "ax1.set_ylabel('Accuracy', fontsize=12)\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_ylim(0, 1)\n",
    "\n",
    "# Plot loss (if available)\n",
    "if 'loss' in history_df_model_4.columns:\n",
    "    ax2.plot(history_df_model_4.index, history_df_model_4['loss'], \n",
    "             label='Model_4 Train Loss', linewidth=2, marker='o', markersize=4)\n",
    "    ax2.plot(history_df_model_4.index, history_df_model_4['val_loss'], \n",
    "             label='Model_4 Val Loss', linewidth=2, marker='s', markersize=4)\n",
    "    ax2.plot(history_df_cnn.index, history_df_cnn['loss'], \n",
    "             label='CNN Train Loss', linewidth=2, marker='^', markersize=4)\n",
    "    ax2.plot(history_df_cnn.index, history_df_cnn['val_loss'], \n",
    "             label='CNN Val Loss', linewidth=2, marker='D', markersize=4)\n",
    "    \n",
    "    ax2.set_title('Model Loss Comparison', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xlabel('Epoch', fontsize=12)\n",
    "    ax2.set_ylabel('Loss', fontsize=12)\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics\n",
    "print(\"=\"*60)\n",
    "print(\"MODEL PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Dense Neural Network (Model_4) Test Accuracy: {test_acc*100:.2f}%\")\n",
    "print(f\"Simple CNN Test Accuracy: {test_acc_cnn*100:.2f}%\")\n",
    "print()\n",
    "\n",
    "# Detailed classification reports\n",
    "for name, y_true, y_pred in models_data:\n",
    "    print(f\"\\nClassification Report - {name}\")\n",
    "    print(\"-\" * 50)\n",
    "    print(classification_report(y_true, y_pred, target_names=genres, digits=3))\n",
    "\n",
    "# Performance comparison bar chart\n",
    "model_names = ['Dense NN\\n(Model_4)', 'Simple CNN']\n",
    "accuracies = [test_acc * 100, test_acc_cnn * 100]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(model_names, accuracies, \n",
    "               color=['skyblue', 'lightcoral'], \n",
    "               edgecolor='black', \n",
    "               linewidth=1.5,\n",
    "               alpha=0.8)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, \n",
    "             f'{acc:.2f}%', ha='center', va='bottom', fontweight='bold', fontsize=12)\n",
    "\n",
    "plt.title('Model Accuracy Comparison', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.ylabel('Test Accuracy (%)', fontsize=12)\n",
    "plt.ylim(0, max(accuracies) * 1.1)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add a horizontal line for average performance\n",
    "avg_acc = np.mean(accuracies)\n",
    "plt.axhline(y=avg_acc, color='red', linestyle='--', alpha=0.7, \n",
    "            label=f'Average: {avg_acc:.2f}%')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Additional insight: Per-class accuracy\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PER-CLASS PERFORMANCE ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for name, y_true, y_pred in models_data:\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(\"-\" * len(name))\n",
    "    \n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    per_class_acc = cm.diagonal() / cm.sum(axis=1) * 100\n",
    "    \n",
    "    for genre, acc in zip(genres, per_class_acc):\n",
    "        print(f\"{genre:15}: {acc:6.2f}%\")\n",
    "\n",
    "# Model Performance Comparison: Dense Neural Network vs. Simple CNN  \n",
    "\n",
    "## Key Observations  \n",
    "\n",
    "1. **Training and Validation Accuracy**  \n",
    "   - **Dense Neural Network (Model_A)** exhibits higher training accuracy compared to the Simple CNN, suggesting better learning capability on the training data.  \n",
    "   - However, the validation accuracy of Model_A is slightly lower than its training accuracy, indicating mild overfitting.  \n",
    "   - **Simple CNN** shows more stable performance with closer alignment between training and validation accuracy, implying better generalization.  \n",
    "\n",
    "2. **Confusion Matrix Analysis**  \n",
    "   - **Dense Neural Network (Model_4)** demonstrates strong performance for genres like `reggae`, `pop`, `metal`, and `classical`, with high true positive rates (e.g., 98 for `reggae`, 102 for `metal`).  \n",
    "   - Misclassifications are observed for `rock` (often confused with `metal` and `reggae`) and `blues` (sometimes misclassified as `jazz` or `metal`).  \n",
    "   - **Simple CNN** struggles with smaller datasets or overlapping features, as seen in lower true positives (e.g., only 6 correct for `jazz` and 10 for `classical`). It also shows higher misclassification rates across genres like `hiphop` and `disco`.  \n",
    "\n",
    "3. **Overall Performance**  \n",
    "   - The Dense Neural Network achieves higher accuracy for most genres, likely due to its capacity to capture complex patterns in the data.  \n",
    "   - The Simple CNN, while less accurate, generalizes better to unseen data (as seen in the validation curves) and may be more efficient for smaller or noisier datasets.  \n",
    "\n",
    "## Conclusion  \n",
    "For tasks requiring high precision (e.g., music genre classification), the **Dense Neural Network** is preferable despite minor overfitting. However, if computational efficiency and generalization are priorities, the **Simple CNN** offers a balanced alternative. Further tuning (e.g., regularization for Model_A or data augmentation for the CNN) could enhance their performance.  \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 568973,
     "sourceId": 1032238,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
